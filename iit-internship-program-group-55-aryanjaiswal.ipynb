{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Processing\n#### Preprocesses data from CSV files, drops specified columns and columns with missing values.\n","metadata":{}},{"cell_type":"code","source":"# Load the dataset 'gee_features_10pct.csv' into a DataFrame called 'dt'\ndt= pd.read_csv('/kaggle/input/maternal-and-child-health-monitoring-in-lmics/gee_features_10pct.csv')\n# Load the sample submission CSV file into a DataFrame called 'smpl_sub'\nsmpl_sub = pd.read_csv('/kaggle/input/maternal-and-child-health-monitoring-in-lmics/sample submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smpl_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the sample submission DataFrame 'smpl_sub' with the 'dt' DataFrame based on the 'DHSID' column\npredict_df = pd.merge(smpl_sub, dt, on='DHSID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns to be dropped from 'dt' DataFrame\ndt = dt.drop(['new_ind', 'index', 'ADM1DHS', 'ADM1FIPS', 'ADM1FIPSNA', 'ADM1NAME', 'ADM1SALBCO', 'ADM1SALBNA', 'ADM1SALNA', 'ALT_DEM', 'ALT_GPS', 'CCFIPS', 'DATUM', 'DHSCC', 'DHSCLUST', 'DHSREGCO', 'DHSREGNA', 'DHSYEAR', 'F21', 'F22', 'F23', 'LATNUM', 'LONGNUM', 'SOURCE', 'URBAN_RURA', 'ZONECO', 'ZONENA','key1','key2','key3'], axis = 1)\npredict_df = predict_df.drop(['Mean_BMI', 'Median_BMI', 'Unmet_Need_Rate', 'Under5_Mortality_Rate', 'Skilled_Birth_Attendant_Rate', 'Stunted_Rate','new_ind', 'index', 'ADM1DHS', 'ADM1FIPS', 'ADM1FIPSNA', 'ADM1NAME', 'ADM1SALBCO', 'ADM1SALBNA', 'ADM1SALNA', 'ALT_DEM', 'ALT_GPS', 'CCFIPS', 'DATUM', 'DHSCC', 'DHSCLUST', 'DHSREGCO', 'DHSREGNA', 'DHSYEAR', 'F21', 'F22', 'F23', 'LATNUM', 'LONGNUM', 'SOURCE', 'URBAN_RURA', 'ZONECO', 'ZONENA','key1','key2','key3'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop columns with missing values from the 'dt' DataFrame\ndt = dt.dropna(axis = 1)\n# Drop columns with missing values from the 'predict_df' DataFrame\npredict_df = predict_df.dropna(axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract 'DHSID' values from the 'dt' DataFrame\nDHSID = np.array(dt['DHSID'])\n# Extract 'DHSID' values from the 'predict_df' DataFrame\nDHSID_sub = np.array(predict_df['DHSID'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the 'DHSID' column from the 'dt' DataFrame\ndt = dt.drop('DHSID', axis = 1)\n# Drop the 'DHSID' column from the 'predict_df' DataFrame\npredict_df = predict_df.drop('DHSID', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Principal Component Analysis (PCA) for Dimensionality Reduction\n### Scaling the Data","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the 'dt' dataset using StandardScaler\nscaler = StandardScaler()\ndt = scaler.fit_transform(dt)\n\n# Initialize PCA\npca = PCA()\n\n# Fit PCA to the scaled data\npca.fit(dt)\n\n# Get explained variance ratios and cumulative variance ratios\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# Plot the cumulative explained variance ratio\nnum_components = len(explained_variance_ratio)\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_components + 1), cumulative_variance_ratio, marker='o', linestyle='-')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.title('Cumulative Explained Variance Ratio vs. Number of Components')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Training Labels\n### Reading Training Labels CSV","metadata":{}},{"cell_type":"code","source":"# Read the training labels CSV file into the 'df_train' DataFrame\ndf_train = pd.read_csv('/kaggle/input/maternal-and-child-health-monitoring-in-lmics/training_label.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform PCA with 2000 components on the 'dt' DataFrame\npca = PCA(n_components=2000)\n\n# Transform the 'predict_df' DataFrame using a previously defined scaler\npredict_sc = scaler.transform(predict_df)\npredict_sc = pd.DataFrame(predict_sc)\n\n# Apply PCA transformation to the 'dt' DataFrame\nX_pca = pca.fit_transform(dt)\n\n# Apply PCA transformation to the 'predict_df' DataFrame\nX_pca_sub = pca.transform(predict_sc)\n\n# Create DataFrames from the PCA-transformed data\ntransformed_df_final = pd.DataFrame(X_pca)\ntransformed_sub_final = pd.DataFrame(X_pca_sub)\n\n# Add 'DHSID' columns to the PCA-transformed DataFrames\ntransformed_df_final['DHSID'] = DHSID\ntransformed_sub_final['DHSID'] = DHSID_sub\n\n# Merge the 'df_train' DataFrame with the PCA-transformed DataFrames based on 'DHSID'\nmerged_df = pd.merge(df_train, transformed_df_final, on='DHSID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_df_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# target columns\ntarget_columns = ['Mean_BMI', 'Median_BMI', 'Unmet_Need_Rate', 'Under5_Mortality_Rate' ,'Skilled_Birth_Attendant_Rate' , 'Stunted_Rate']\n\n# Create an empty dictionary to store RMSE values for each target column\nrmse_dict = {}\n\nfor target_col in target_columns:\n    merged_df_dropped = merged_df[merged_df[target_col].notna()]\n    col_names = list(transformed_df_final.columns)\n    col_names.remove('DHSID')\n    X_value = merged_df_dropped[col_names]\n    Y_value = merged_df_dropped[target_col]\n\n    x_train, x_test, y_train, y_test = train_test_split(X_value, Y_value, test_size=0.2, random_state=42)\n    len(x_train), len(x_test), len(y_train), len(y_test)\n\n    elastic_net = ElasticNet()\n\n    # Define a grid of hyperparameter values to search\n    param_grid = {\n        'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n        'l1_ratio': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    }\n\n    # Create GridSearchCV instance\n    grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n\n    # Fit the GridSearchCV\n    grid_search.fit(x_train, y_train)\n    best_model = grid_search.best_estimator_\n    \n    # Print the best hyperparameters and corresponding score\n    print(f\"Best Hyperparameters for {target_col}:\", grid_search.best_params_)\n    print(f\"Best Score for {target_col}:\", -grid_search.best_score_)\n    \n    preds = best_model.predict(x_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    \n    # Store RMSE in the dictionary\n    rmse_dict[target_col] = rmse\n\n# Calculate the average RMSE across all target columns\naverage_rmse = np.mean(list(rmse_dict.values()))\n\n# Print the RMSE values for each target column and the average RMSE\nfor target_col, rmse in rmse_dict.items():\n    print(f\"RMSE for {target_col}: {rmse}\")\n\nprint(\"Average RMSE across all columns:\", average_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}